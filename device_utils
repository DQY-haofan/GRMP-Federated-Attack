# device_utils.py - ç»Ÿä¸€çš„è®¾å¤‡ç®¡ç†å·¥å…·
import torch
import os

try:
    import torch_xla
    import torch_xla.core.xla_model as xm
    import torch_xla.distributed.parallel_loader as pl

    TPU_AVAILABLE = True
except ImportError:
    TPU_AVAILABLE = False
    xm = None
    pl = None


class DeviceManager:
    """ç»Ÿä¸€ç®¡ç†TPU/GPU/CPUè®¾å¤‡é€‰æ‹©å’Œä¼˜åŒ–"""

    def __init__(self):
        self.device_type, self.device = self._detect_device()
        print(f"ğŸš€ Using device: {self.device_type}")

        # è®¾ç½®ä¼˜åŒ–å‚æ•°
        if self.device_type == 'cuda':
            torch.backends.cudnn.benchmark = True
            torch.backends.cudnn.deterministic = True

    def _detect_device(self):
        """è‡ªåŠ¨æ£€æµ‹æœ€ä½³å¯ç”¨è®¾å¤‡"""
        # ä¼˜å…ˆçº§: TPU > GPU > CPU

        # 1. æ£€æŸ¥TPU
        if TPU_AVAILABLE and 'COLAB_TPU_ADDR' in os.environ:
            try:
                device = xm.xla_device()
                print("âœ… TPU detected and initialized")
                return 'tpu', device
            except Exception as e:
                print(f"âŒ TPU initialization failed: {e}")

        # 2. æ£€æŸ¥GPU
        if torch.cuda.is_available():
            device = torch.device('cuda')
            gpu_name = torch.cuda.get_device_name(0)
            print(f"âœ… GPU detected: {gpu_name}")
            return 'cuda', device

        # 3. é»˜è®¤CPU
        print("âš ï¸  No accelerator found, using CPU")
        return 'cpu', torch.device('cpu')

    def get_device(self):
        """è·å–è®¾å¤‡å¯¹è±¡"""
        return self.device

    def is_tpu(self):
        """æ£€æŸ¥æ˜¯å¦ä½¿ç”¨TPU"""
        return self.device_type == 'tpu'

    def is_gpu(self):
        """æ£€æŸ¥æ˜¯å¦ä½¿ç”¨GPU"""
        return self.device_type == 'cuda'

    def move_to_device(self, tensor_or_model):
        """å°†å¼ é‡æˆ–æ¨¡å‹ç§»åŠ¨åˆ°è®¾å¤‡"""
        if self.is_tpu():
            return tensor_or_model.to(self.device)
        else:
            return tensor_or_model.to(self.device)

    def optimizer_step(self, optimizer):
        """æ‰§è¡Œä¼˜åŒ–å™¨æ­¥éª¤ï¼ˆTPUéœ€è¦ç‰¹æ®Šå¤„ç†ï¼‰"""
        if self.is_tpu():
            xm.optimizer_step(optimizer)
        else:
            optimizer.step()

    def mark_step(self):
        """TPUéœ€è¦çš„åŒæ­¥æ­¥éª¤"""
        if self.is_tpu():
            xm.mark_step()

    def get_world_size(self):
        """è·å–åˆ†å¸ƒå¼è®­ç»ƒçš„è¿›ç¨‹æ•°"""
        if self.is_tpu():
            return xm.xrt_world_size()
        else:
            return 1

    def get_ordinal(self):
        """è·å–å½“å‰è¿›ç¨‹çš„æ’å"""
        if self.is_tpu():
            return xm.get_ordinal()
        else:
            return 0

    def create_parallel_loader(self, data_loader):
        """åˆ›å»ºå¹¶è¡Œæ•°æ®åŠ è½½å™¨ï¼ˆTPUä¼˜åŒ–ï¼‰"""
        if self.is_tpu():
            return pl.ParallelLoader(data_loader, [self.device])
        else:
            return data_loader

    def reduce_mean(self, tensor):
        """è·¨è®¾å¤‡æ±‚å¹³å‡ï¼ˆç”¨äºåˆ†å¸ƒå¼è®­ç»ƒï¼‰"""
        if self.is_tpu():
            return xm.mesh_reduce('reduce_mean', tensor, lambda x: sum(x) / len(x))
        else:
            return tensor


# å…¨å±€è®¾å¤‡ç®¡ç†å™¨å®ä¾‹
device_manager = DeviceManager()
